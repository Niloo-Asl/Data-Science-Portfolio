---
title: "Classification model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Required libraries

```{r}
library(readr)
library(rpart)
library(rattle)
library(adabag)
```

#### Importing Data

```{r}
df <- read_delim("data.csv", ";", escape_double = FALSE, trim_ws = TRUE)
```

#### Cleaning the data

Removing extra characters in column name by changing the column names:

```{r}
colnames(df)= gsub('"','',colnames(df))
```

Removing extra characters in each column values:

```{r}
df$age=gsub('"','',df$age)
df$job=gsub('"','',df$job)
df$marital=gsub('"','', df$marital)
df$education=gsub('"','',df$education)
df$default=gsub('"','', df$default)
df$housing=gsub('"','', df$housing)
df$loan=gsub('"','', df$loan)
df$contact=gsub('"','', df$contact)
df$month=gsub('"','', df$month)
df$poutcome=gsub('"','', df$poutcome)
df$y=gsub('"','', df$y)
```

Duplicate verification
```{r}
ifelse(nrow(df[(duplicated(df)) , ])==0,"No duplicate", "There are duplicated in the table")
```

Average and Median of age of the Clients accounts' balance :
```{r}
summary(df)
```
Summary result shows that age variable is character so I convert it to numeric
```{r}
df$age=as.numeric(df$age)
```

Average and Median of age of the Clients accounts' balance :
```{r}
paste("Avrage age of the clients is: ",round(mean(df$age),1), " years")
paste("Median of age of clients is: ",median(df$age), "years")
```

Changing the name of the variable representing if the client has subscribed to direct deposit or no to ‘deposit’
```{r}
names(df)[grep("^y", colnames(df))]="deposit"
```

Creating the data_deposit data table which only include customers with a balance higher than 0.
```{r}
data_deposit=subset(df, balance > 0)
```

### Designing a classification tree on the data_deposit data using all the explanatory variables:
**Note** This classification tree is a Gini-based tree, with a maximum depth of 5 and a minimum number of 50 observations in the terminal leaves.
```{r}
rpart_deposit=rpart(deposit~., data_deposit, parms = list(split='gini'), control=list(maxdepth=5, minsplit=50) , method ='class')
```

### The classification tree graph
```{r}
par(mar=c(0,2,2,0))
plot(rpart_deposit)
text(rpart_deposit)
```

### The classification tree fancy graph
```{r}
fancyRpartPlot(rpart_deposit, caption = NULL)
```

### Interpretation of the first terminal leaf :
```{r}
rpart_deposit
```

___poutcome=failure, other, unknown 32226 2072 no (0.93570409 0.06429591)___

 The parent leaf of this terminal leaf is a 2-branch leaf :
 
 -	poutcome = failure, other, unknown (the terminal leaf that I study here)
 -	poutcome = success

This terminal leaf has 32226 observations of which 2072 have a deposit value of « yes » and 30152 (32226-2072) observations have a deposit value of « no ». This means that 2072 values of deposit are mis-classified.

So, the majority value of deposit in this leaf is « yes » which make (30152/32226) 93.57% of observations of this leaf.

As a result, with a probability of 93.6% of "yes" the judgment of this leaf is « yes » and 6.4% (2072/32226) of the observations in this leaf are mis-classified.

### Two explanatory variables having the greatest importance in the construction of the tree:
```{r}
rpart_deposit$variable.importance[order(rpart_deposit$variable.importance, decreasing = TRUE)]
```

"duration" with importance value of 1180 and "pourcome" with importance value of 790

### Natural classification error rate in data_deposit data table:
```{r}
table(data_deposit$deposit)/nrow(data_deposit)
```
The classification error rate is __12.6%__

### Designing Boosting model with 50 trees and a maximum depth of 4:
### Designing Bagging model with 50 trees and a maximum depth of 4:
### Designing Rnadon Forest model with 50 trees and a maximum depth of 4 and a random subset of 5 variables: