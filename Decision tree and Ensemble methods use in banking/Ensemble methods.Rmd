---
title: "Classification model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Required libraries

```{r}
library(readr)
library(rpart)
library(rattle)
library(adabag)
library(randomForest)
```

#### Importing Data

```{r}
df <- read_delim("data.csv", ";", escape_double = FALSE, trim_ws = TRUE)
```

#### Cleaning the data

Removing extra characters in column name by changing the column names:

```{r}
colnames(df)= gsub('"','',colnames(df))
```

Removing extra characters in each column values:
```{r}
cols <- c("age", "job", "marital","default","education","housing","loan","contact","month","poutcome","y")
df[col]=lapply(df[col], function(x) gsub("'","",x))
```

Duplicate verification
```{r}
ifelse(nrow(df[(duplicated(df)) , ])==0,"No duplicate", "There are duplicated in the table")
```

Average and Median of age of the Clients accounts' balance :
```{r}
summary(df)
```
Summary result shows that age variable is character so I convert it to numeric
```{r}
df$age=as.numeric(df$age)
```

Average and Median of age of the Clients accounts' balance :
```{r}
paste("Avrage age of the clients is: ",round(mean(df$age),1), " years")
paste("Median of age of clients is: ",median(df$age), "years")
```

Changing the name of the variable representing if the client has subscribed to direct deposit or no to ‘deposit’
```{r}
names(df)[grep("^y", colnames(df))]="deposit"
```

Creating the data_deposit data table which only include customers with a balance higher than 0.
```{r}
data_deposit=subset(df, balance > 0)
```

### Designing a classification tree on the data_deposit data using all the explanatory variables:
**Note** This classification tree is a Gini-based tree, with a maximum depth of 5 and a minimum number of 50 observations in the terminal leaves.
```{r}
rpart_deposit=rpart(deposit~., data_deposit, parms = list(split='gini'), control=list(maxdepth=5, minsplit=50) , method ='class')
```

### The classification tree graph
```{r}
par(mar=c(0,2,2,0))
plot(rpart_deposit)
text(rpart_deposit)
```

### The classification tree fancy graph
```{r}
fancyRpartPlot(rpart_deposit, caption = NULL)
```

### Interpretation of the first terminal leaf :
```{r}
rpart_deposit
```

___poutcome=failure, other, unknown 32226 2072 no (0.93570409 0.06429591)___

 The parent leaf of this terminal leaf is a 2-branch leaf :
 
 -	poutcome = failure, other, unknown (the terminal leaf that I study here)
 -	poutcome = success

This terminal leaf has 32226 observations of which 2072 have a deposit value of « yes » and 30152 (32226-2072) observations have a deposit value of « no ». This means that 2072 values of deposit are mis-classified.

So, the majority value of deposit in this leaf is « yes » which make (30152/32226) 93.57% of observations of this leaf.

As a result, with a probability of 93.6% of "yes" the judgment of this leaf is « yes » and 6.4% (2072/32226) of the observations in this leaf are mis-classified.

### Two explanatory variables having the greatest importance in the construction of the tree:
```{r}
rpart_deposit$variable.importance[order(rpart_deposit$variable.importance, decreasing = TRUE)]
```

"duration" with importance value of 1180 and "pourcome" with importance value of 790

### Natural classification error rate in data_deposit data table:
```{r}
table(data_deposit$deposit)/nrow(data_deposit)
```
The classification error rate is __12.6%__

## Ensemble Methods
For Ensemble Methods, the variables of type character must be converted into factors
```{r}
cols <- c("deposit", "job", "marital","education","housing","loan","contact","month","poutcome","default")
data_deposit[cols] <- lapply(data_deposit[cols], factor)
```
And the table should be dataframe
```{r}
data_deposit=data.frame(data_deposit)
```

### Designing Bagging model with 50 trees and a maximum depth of 4:
```{r}
bagging_deposit=bagging(deposit~.,data_deposit,mfinal=50, control=rpart.control(maxdepth=4))
```
Bagging Misclassification rate (MR)
```{r}
SUMM=sum(diag(table(bagging_deposit$class, data_deposit$deposit)))
MR_bagging=1-(SUMM/nrow(data_deposit))
```

### Designing Boosting model with 50 trees and a maximum depth of 4:
```{r}
boosting_deposit=boosting(deposit~.,data_deposit,mfinal=50, control=rpart.control(maxdepth=4))
```
Boosting Misclassification Rate (MR)
```{r}
SUMM=sum(diag(table(boosting_deposit$class, data_deposit$deposit)))
MR_boosting=1-(SUMM/nrow(data_deposit))
```

### Designing Rnadon Forest model with 50 trees and a maximum depth of 4 and a random subset of 5 variables:
```{r}
RF_deposit=randomForest(deposit~.,data_deposit,ntree=50,mtry=5)
```
Random Forest Misclassification Rate (MR)
```{r}
SUMM=sum(diag(table(RF_deposit$predicted, data_deposit$deposit)))
MR_RF=1-(SUMM/nrow(data_deposit))
```

Comparing the mis-classification rates of Bagging, boosting and Random Forest methods to decide which one performs better:
```{r}
paste("Bagging misclassification rate is : ", MR_bagging)
paste("Boosting misclassification rate is : ", MR_boosting)
paste("Random Forest misclassification rate is : ", MR_RF)
```
The misclassification rates of all 3 models are smaller than the classification error rate of the dataset (12.6%). This means that the 3 models are efficient. But the misclassification rate of the Boosting model is the smallest of all; which means that Boosting model is the most efficient models.